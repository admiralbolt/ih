#!python
"""
A helper script as a workaround for >1000 column sqlite joins.
The script itself walks through the current directory, and
creates an sql file that will join all possible databases.
"""
import os
import argparse

parser = argparse.ArgumentParser(description = "Creates an sql script to join many databse files.")
parser.add_argument("--prefix", dest="prefix", default="", help="Optional prefix to match for db names.")
parser.add_argument("--output", dest="output", default="combine.sql", help="Output file name.")
args = parser.parse_args()

# The required lines to write for each database
lines = [
"begin;",
"insert into images select * from to_merge.images;",
"commit;",
"detach to_merge;"
]

# Writes the resulting combine.sql
with open(args.output, "w") as wh:
    for root, dirs, files in os.walk("."):
        for f in files:
            # Matches all db files
            if f[-2:] == "db":
                if not args.prefix or f[:-len(args.prefix)] == args.prefix:
                    first_line = ["attach \"%s\" as to_merge;" % (f,)]
                    write_lines = first_line + lines
                    for line in write_lines:
                        wh.write(line + "\n")
                    wh.write("\n")
        break

"""
You may want to optionally filter based on the type of the data if different
imtypes have different processing results.
After creating the combine.sql file, you can aggregate data the following way:
1. Copy a sample database to maintain original structure.
    > cp fluosv0.db fluosv_all.db
2. Load up the final database in sqlite:
    > sqlite3 fluosv_all.db
3. Within sqlite, read in the created sql file:
    > .read combine.sql
"""
